%!TEX root = main.tex  
\section{Usability Evaluation}
In this section we will describe the results and observations made in the usability test. Before we conducted the usability test we created a usability test plan: Appendix \ref{chap:usability}, where all the different parts of the usability test is described in detail.

The usability test was conducted with four participants, recruited from a software development project course at NTNU(IT2901)\footnote{\url{http://www.ntnu.edu/studies/courses/IT2901}}. The application was evaluated in it's production stage, using the most-recent version at the time. User-interaction with the PeacefulBanana application was done through an Internet Browser(Google Chrome\footnote{\url{http://www.google.com/intl/no/chrome/browser/}}). The users were recruited , and the test was ran with the latest production version of the application. 

Users answered an entrance questionnaire, in order to collect demographic information. During the usability test we took notes of the user's problems and concerns. When the test was completed, participants could comment with suggestions on improvement of the application or design. Participants were given a \emph{System Usability Scale} form to answer\citep{brooke1996sus}, which consisted of 10 questions designed to measure user satisfaction. 

\subsection{Context}
\label{subsec:context}
The usability test simulated the two scenarios identified in Section \ref{sec:scenarios} and was set in the context of these. We conducted the usability test in order to answer several important questions, regarding these scenarios: 
\begin{itemize}
	\item Is the application easy to use, and can users achieve their goals in a timely manner?
	\item Identify the relationship users have with the aspect of reflection and sharing personal experiences.
	\item Does the tool present data in a way that triggers reflection for the user?
\end{itemize}
Feedback from the usability test was used to further aid design and help identify problem areas that might cause problems for potential users. Participants were also familiar with reflection, so we hoped to collect valuable input regarding this concept.

\subsection{Participants}
As mentioned we had four participants in the usability test. Typically, three to five test participants is the optimal number for most usability studies\citep{nielsen1993mathematical}. As the PeacefulBanana tool is intended to be used with developers with a computer-science background, participants were master students on the Computer Science field at NTNU. Participants had experience with usability testing and also experience with the notion of reflection, from earlier projects using an agile methodology process.
Responsibilities of participants were to attempt to complete a set of representative task scenarios presented to them in as efficient and timely a manner as possible, and to provide feedback regarding the usability and acceptability of the application. The participants were directed to provide honest opinions regarding the usability of the application.

\subsection{Procedure}
The usability test took place in a private room at the university. A computer with the PeacefulBanana web application was used in a typical working environment. The participants interaction with the application was monitored by the facilitator seated in the same room.
The facilitator briefed the participants on the web application and instructed the participants that we are evaluating the application, rather than evaluating the participant. Participants signed an informed consent that acknowledges: \emph{the participation is voluntary, that participation can cease at any time, and that their privacy of identification will be kept safe}. Consent form can be seen in Appendix \ref{app:consentform}.

The facilitator explained that the amount of time taken to complete the test task will be measured and that exploratory behavior outside the task flow should not occur until after task completion. At the start of each task, the participant read aloud the task description from the printed copy and began the task, and time-on-task measurement began simultaneously.
The facilitator instructed the participant to \emph{‘think aloud’} so that the facilitator could observe and take notes of user behavior and user comments.
After all task scenarios are attempted, the participant completed the post-test satisfaction questionnaire derived from the SUS scale\footnote{System Usability Scale}.

\subsection{Roles}
For our usability test we had two roles, in addition to the test participants:
\subsubsection{Facilitator}
	\begin{itemize}
		\item Provides overview of study to participants.
		\item Defines usability and purpose of usability testing.
		\item Responds to participant's requests for assistance.
	\end{itemize}
\subsubsection{Test Observer}
	\begin{itemize}
		\item Silent observer
		\item Takes notes of identified problems, concerns, coding bugs, and procedural errors.
		\item Serve as note takers.
	\end{itemize}

\subsection{Ethics}
All persons involved with the usability test were required to adhere to the following ethical guidelines:
\begin{itemize}
	\item The performance of any test participant must not be individually attributable. Individual participant's name should not be used in reference outside the testing session.
	\item A description of the participant's performance should not be reported. 
\end{itemize}

\subsection{Usability Tasks}
The usability tasks were derived from our scenarios, described in Section \ref{sec:scenarios}. Due to the short time for which each participant was available, the tasks used were the most common and relatively complex of available functionality. The tasks were identical for all participants in the study.
The application was tested in a development environment and databases were populated during use, and not pre-populated. This ensured a similar experience as to what the users would get when they first use PeacefulBanana in a real-life setting. The web application was run on a local computer, and not on a dedicated server as it will when deployed in production. This and the possible extra overhead from running the application in development mode, may have an impact on performance slightly in a negative way.

\subsubsection{Task context}
PeacefulBanana is a tool intended to promote reflection and allow for revisiting and learning from previous experiences. PeacefulBanana integrates with and collects data from the version-control system GitHub.
\subsubsection{Scenario 1 tasks:}
Here are the tasks participants were to solve related to Scenario 1:
\begin{itemize}
	\item Task 1: You start the application for the first time, and want to login, link your account with Github and set an active repository.
	\item Task 2: 
		\begin{itemize}
			\item Task 2.1: View your notifications.
			\item Task 2.2: Find the \emph{“Congratulations”} notification and archive it. Find the archive and see if the notification was indeed archived
		\end{itemize}
	\item Task 3:
		\begin{itemize}
			\item Task 3.1: Find the \emph{“Reminder: Daily Reflection”} note and perform the daily summary.
			\item Task 3.2: Find a daily summary note and share it. Verify that is has indeed been shared.
			\item Task 3.3: Find your mood graph
		\end{itemize}
\end{itemize}

\subsubsection{Scenario 2 tasks:}
Here are the tasks participants were to solve related to Scenario 2:
\begin{itemize}
\item Task 4:
	\begin{itemize}
		\item Task 4.1: Create a team with the name \emph{“Tuttifrutti”} and your previously chosen repository.
		\item Task 4.2: Find your created team and set it to active.
		\item Task 4.3: Identify the members on your team and their role.
	\end{itemize}
\item Task 5:
	\begin{itemize}
		\item Task 5.1: Find all your repositories’ milestones.
		\item Task 5.2: Identify your overdue milestones.
		\item Task 5.3: Find your repositories issues.
		\item Task 5.4: Find issue \#17 . Find the status of this issue, when it was opened and when it was closed.
	\end{itemize}
\item Task 6:
	\begin{itemize}
		\item Task 6.1: Generate a tagcloud for your current chosen repository.
		\item Task 6.2: Identify the most used tag for your team and yourself.
		\item Task 6.3: Find the commit impact for your repository.
	\end{itemize}
\end{itemize}

\subsection{Usability Metrics}
Usability metrics refers to user performance measured against specific performance goals necessary to satisfy usability requirements. Scenario completion success rates, error rates, and subjective evaluations was collected, additionally Time-to-completion/Time-on-task was also collected.

\subsubsection{Task Completion}
Each task requires that the participant obtains or inputs specific data that would be used in course of a typical task. The task is noted as \emph{completed} when the participant indicates the task's goal has been obtained (whether successfully or unsuccessfully).  If a participant requires assistance in order to achieve a correct output then the task will be noted as a critical error and the overall completion rate for the task will be affected.

\subsubsection{Completion Rate}
A completion rate of 100\% is the goal for each task in this usability test. \\
Completion rate is the percentage of test participants who successfully complete the task without critical errors, an \emph{output} that is correct. If a participant requires assistance in order to achieve a correct output then the task will be scored as a critical error and the overall completion rate for the task will be affected.

\subsubsection{Critical Errors}
A critical error is an error that results in an incorrect or incomplete outcome. Participants may or may not be aware that the task goal is incorrect or incomplete. In general, critical errors are unresolved errors during the process of completing the task or errors that produce an incorrect outcome.

\subsubsection{Non-Critical Errors}
Non-critical errors are errors that are recovered from by the participant or, if not detected, do not result in processing problems or unexpected results. Although non-critical errors can be undetected by the participant, when they are detected they are often frustrating to the participant.

\subsubsection{Subjective Evaluations}
Opinions of participators regarding specific tasks, time to perform each task, features, and functionality was collected. At the end of the usability test, participants rated their satisfaction with the overall system. Combined with the interview/debriefing session. 

\subsubsection{Task Completion Time(time on task)}
The time to complete a task is referred to as \emph{"time on task"}. It is measured from the time the person begins the task to the time the participant indicates completion.

\subsection{General Usability Goals}
The next section describes the usability goals for \emph{PeacefulBanana} in context of the usability metrics. First the general usability test objectives are described, and what usability metric results we aimed for.\\ 
The goals of usability testing the PeacefulBanana application included establishing a baseline of user performance, validating user performance measures, and identifying potential design issues that needed to be addressed in order to improve efficiency, usability and end-user satisfaction. \\
The general usability test objectives were:
\begin{itemize}
	\item Identify possible problems or breakdowns in the design\citep{wright1989evaluation} early on in the design process. Sources of such breakdowns may include:
		\begin{itemize}
			\item Navigation errors – failure to locate functions, excessive actions to complete a function, failure to follow recommended screen flow.
			\item Presentation errors – failure to locate and properly act upon desired information in screens, selection errors due to labeling ambiguities.
			\item Control usage problems – improper tool bar or entry field usage.
		\end{itemize}
	\item Exercise the PeacefulBanana application under controlled test conditions with representative users, which here are individuals with a background in Computer-Science
	. Data will be used to assess whether usability goals regarding an effective, efficient, and well-received user interface have been achieved.
	\item Establish baseline user performance and user-satisfaction levels of the user interface for future usability evaluations.
\end{itemize}
Typical problems identified in a usability test would be text representations or the placement of design elements, that are not intuitive for the user during use. It would be a concern if the user can't figure out how to use certain features of the application. Identifying these problems as early as possible will lead to a better end result. 

Secondly an objective of the usability test was to identify how users act and think about their daily experiences, how they react to the notion of reflecting on them and if sharing their private thoughts is a problem. 

\subsection{Problem Severity}
In order to analyze collected data from the usability test, identified issues were classified by issue severity. This issue severity is a combination of the impact of the issue and the frequency of users experiencing the issue during the test. 
\subsubsection{Problem Severity Classification}
	\begin{itemize}
		\item High severity: High impact problems that often prevent a user from correctly completing a task. Reward for resolution is reduced redevelopment costs.
		\item Medium severity: Either moderate problems with low frequency or low problems with moderate frequency; these are minor annoyance problems faced by a number of participants. Reward for resolution is typically exhibited in reduced time on task and increased data
		\item Low severity: Low impact problems faced by few participants; there is low risk to not resolving these problems. Reward for resolution is typically exhibited in increased user satisfaction.
	\end{itemize}

\subsection*{Pilot Test}
After finalizing the usability test plan: Appendix \ref{chap:usability}, a pilot test was conducted prior to the usability-test\citep{usabilitygov}. The pilot test allows for an evaluation of the test plan itself and the questionnaires before doing the actual usability test. This means the pilot test is a \emph{"test of the test"}, where the goal is to evaluate and verify that the test itself is well-formulated.We chose a fellow student as our pilot-tester, in order to check whether the test script was clear, that the tasks were appropriately difficult, and that the data collected can be meaningfully analyzed. 
It also allows the "tester" to practice the execution and guidance, before actually performing the tests. \\
In the pilot test for PeacefulBanana, all of the aspects above were evaluated and a few tweaks were made to the tests, making it more streamlined. Also a few, smaller bugs in the application were discovered and fixed. The test introduction was rewritten, since the pilot-tester showed some confusion in a few of the tasks. 

\newpage
\subsection{Usability Test Results}
We conducted an on site usability test using a production version of PeacefulBanana, located on the test administrators local server. One tester took notes of comments, facial expressions and navigation choices. The administrator acted as guidance during the test. The sessions captured each participants navigational choices, task completion rates, comments, overall satisfaction ratings, questions and feedback.
The usability test was conducted in a private lab-room at NTNU on November 10th. The purpose of the test was to assess the usability of the web application design, information flow, information architecture and the effects of sharing personal reflection notes. The findings acted as valuable feedback to our delivery cycle, and were used for improving the design of the application. 
Four participants attended the test. Each individual session lasted approximately twenty minutes.
This section contains the participant feedback, satisfactions ratings, task completion rates, ease or difficulty of completion ratings, time on task, errors, and recommendations for improvements.
\subsection*{Task Completion Success Rate}
This section presents the task completion rates for our two scenarios. Figure \ref{scenario1completionrate} shows the completion rate table for Scenario 1, and Figure \ref{scenario2completionrate} shows the completion rate table for Scenario 2. 
\subsubsection{Scenario 1 completion rates:}
All participants successfully completed (100\% Completion rate):
\begin{itemize}
	\item Task 1 - Start the application and set active repository. 
	\item Task 2.1 - View notifications. 
	\item Task 3.1 - Find the \emph{Reminder: Daily reflection} note and perform the daily summary. 
	\item Task 3.2 - Find and share the daily reflection note.
\end{itemize}
For Task 2.2(Find and archive congratulations notification) and Task 3.3(Find mood graph), 3 out of 4 participants completed the tasks(75\% Completion rate). 
\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{scenario1completionrate}
    \caption{Scenario 1 Completion Rate}
    \label{scenario1completionrate}
\end{figure}

\textbf{Scenario 2 completion rates:}
All participants successfully completed (100\% Completion rate):
\begin{itemize}
	\item Task 4.1 - Create a team. 
	\item Task 4.2 - Set active team. 
	\item Task 4.3 - Identify team members. 
	\item Task 5.2 - Identify overdue milestones. 
	\item Task 5.4 - Find issue \#17
	\item Task 6.1 - Generate tag cloud
	\item Task 6.2 - Identify most used personal tags and team tags
	\item Task 6.3 - Find commit impact
\end{itemize}
3 out of 4 participants(75\%) successfully completed Task 5.1(Find all milestones for your repository), while 2 out 4(50\%) successfully completed Task 5.3 (Find your repositories issues). 

\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{scenario2completionrate}
    \caption{Scenario 2 Completion Rate}
    \label{scenario2completionrate}
\end{figure}

\subsection*{Time on task}
Time on task for each participant was recorded. Some tasks were inherently more difficult to complete than others and is reflected by the average time on task. Time on task results for Scenario 1 is presented in Figure \ref{timeontaskscenario1}, and time on task results for Scenario 2 is presented in Figure \ref{timeontaskscenario2}.
\subsubsection{Time on task - Scenario 1}
\begin{itemize}
	\item \textbf{Task 1(Start the application and set active team/repository)} showed a high average time on task. The main reason behind this was the authorization with GitHub which required users to login and authorize on the external GitHub.com page.
	\item \textbf{Task 2.1(View notifications)} showed a very similar time on task by the participants, the same can be seen on \textbf{Task 2.2(Find and archive notification)} although the time by each participant was over 80 seconds.
	\item \textbf{Task 3.1(Find reminder and perform daily reflection)} took the longest time to complete(average 222 seconds). However this was to be expected, as the daily reflection note requires participants to reflect on their work, and usually lasts for 2-5 minutes. This task also had the largest range in completion time, ranging from 204 seconds to 272 seconds. \textbf{Task 3.2(Find and share reflection note)} and \textbf{Task 3.3(Find mood graph)} participant time on task averaged 55 and 43 seconds. 
\end{itemize}
\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{timeontaskscenario1}
    \caption{Time on task Scenario 1}
    \label{timeontaskscenario1}
\end{figure}
\subsubsection{Time on task - Scenario 2}
\begin{itemize}
	\item \textbf{Task 4.1(Create a team)} showed the longest completion time in Scenario 2. The main reason behind this was the amount of data that needed to be collected from GitHub and stored in the PeacefulBanana database. The task also showed the longest range in completion time, from 185 seconds to 310 seconds. The reason behind this large gap was mainly the difference in amount of data present in the GitHub repositories they chose to create a team for. \textbf{Task 4.2(Set active team) showed no major changes in completion time, and the same can be seen in \textbf{Task 4.3(Identify team members)}}
	\item \textbf{Task 5.1(Find all milestones)} showed that 3 out of 4 participants had very similar completion times(45, 52 and 51 seconds), but the average was increased by that the last participant had a completion time of 95 seconds. \textbf{Task 5.2(Identify overdue milestones)} showed very similar completion times, while \textbf{Task 5.3(Find repository issues)} had one participant at 71 seconds, while the rest used an average of 50 seconds. \textbf{Task 5.4(Find issue \#17)} showed an average of 75 seconds, with no significant differences. 
	\item \textbf{Task 6.1(Generate tag cloud)} averaged on 50 seconds, \textbf{Task 6.2(Identify most used individual and team tags)} averaged 41 seconds and \textbf{Task 6.3(Find commit impact)} averaged at 36 seconds, all with no significant difference in completion time. 
\end{itemize}
\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{timeontaskscenario2}
    \caption{Time on task Scenario 2}
    \label{timeontaskscenario2}
\end{figure}

\subsection{Summary of Data}
The number of errors participants made while trying to complete the tasks were captured and recorded. Critical errors leads to participant failing in completing scenario, while non-critical errors is an error that does not prevent successful completion of the scenario. These errors along with task completions and time on task average for each task are represented in Figure \ref{datasummary}.
Low completion rate, occurrence of critical-errors and high time on task are highlighted in red. 
\begin{figure}[H]
    \centering
        \includegraphics[width=\textwidth]{datasummary}
    \caption{Summary of Data}
    \label{datasummary}
\end{figure}

\subsubsection{Overall Metrics}
After completing the usability session, participants were given a \emph{System Usability Scale} form to answer\citep{brooke1996sus}. The results from the SUS-scale can be seen in Figure: \ref{posttaskoverall}. 

All participants agreed(i.e., agree or strongly agree) that they would use the application frequently and that the application was easy to use. All participants(100\%) also felt confident when using the application. The majority of the participants(75\%) felt the functions in the application were well integrated, and that most people would learn to use the system very quickly. Half of the participants(50\%) agreed that there were inconsistencies in the system, which were mainly related to a more clear distinction between milestone related issues and repository issues. None of the participants(0\%) found the system unnecessarily complex or that it was cumbersome to use. Further none of the participants felt users would need support of a technical person to use the system(based on the intended user group) or that they needed to learn a lot before getting going with the system. The participants mentioned the quick start guide as a possible look-to document in case of trouble.  
\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{posttaskoverall}
    \caption{Post task}
    \label{posttaskoverall}
\end{figure}

\subsubsection{Reflection and sharing}
\label{subsubsec:reflection}
Participants were also asked to answer the questions identified in Section \ref{subsec:context}
\begin{itemize}
	\item Is the application easy to use, and can users achieve their goals in a timely manner?
\end{itemize}
Feedback here was that participants were satisfied with the ease of use as can be seen above, also time-on-task show that participants were mostly quite similar in the solving of tasks, and very few spikes. 
\begin{itemize}
	\item Identify the relationship users have with the aspect of reflection and sharing personal experiences.
\end{itemize}
On this context, participants answered that reflecting on their experiences is something they often do, but they don't collect them and thus forgets exactly what the experience was about. The application helped solve this problem by prompting and allowing users to reflect and then store it for later use. 

When it comes to sharing, all participants were positive to this, although they strongly emphasized the need for an \emph{unshare} functionality. The missing ability to unshare these notes after hand could make them reluctant to share them in the first place, since when first shared it was always shared. One participant mentioned the possibility of letting notes be editable and share/unshareable for a specific time period, f.ex 24 hours, where afterwards they would be locked for editing.  
\begin{itemize}
	\item Does the tool present data in a way that triggers reflection for the user?
\end{itemize}
Participants responded that the tag-cloud and questions in the daily reflection note triggered them to reflect on experiences. Actually reading the questions in their mind, helped them to reflect on the experiences, instead of just having an empty text-field could lead to random thoughts being collected and not actually triggering reflection. The commit impact graph was mentioned as less-helpful as it didn't really justify the amount of work done. 
	
Participants also provided feedback for what they liked the most and least about the application, and recommendations for improving the application. 
\subsubsection*{Most liked}
The participants liked the design of the applicincation and that it was able to synchronize with their GitHub repositories automatically, without them having to worry about it. The personal tag-cloud and the ability to compare it directly with the team's tag cloud was also a joint feedback.
\subsubsection*{Least liked}
Participants commented that the way notifications were given was not optimal, and the registration process was a bit tedious.
\subsection{Recommendations}
In addition to the feedback gathered from section \ref{subsubsec:reflection}, this section presents proposed changes and their justifications derived from the participant success rate, behaviors, and comments. The identified recommendations will improve the overall ease of use and address the areas where participants experienced problems or found the interface/information architecture unclear.
Feedback on recommendations on improvement was primarily to streamline the registration process. Also the participants commented that issues connected to a specific milestone, should be more visibly separated from issues connected to the whole repository. 

\subsection*{Implemented Changes}
This section presents the proposed changes that were implemented into the PeacefulBanana application. 
\subsubsection*{Task 1: Start application, login, link account with GitHub and set an active repository}
This recommendation, it's justification and it's priority(Severity) is presented in Figure \ref{task1improvement}.
The recommendation concerned a proposed streamlining of the registration process. This was applied to the application in the way that we changed to a larger and more stable mail-server provider, in order to reduce the significance of a slow response time from external providers. The registration process itself was also improved by reducing amount of clicks in the process. 
\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{task1improvement}
    \caption{Task 1 changes and justification}
    \label{task1improvement}
\end{figure}

\subsubsection*{Task 2.2: Find the \emph{Congratulations} notification and archive it. Verify activation.}
This recommendation, it's justification and it's priority(Severity) is presented in Figure \ref{task22improvement}. 
Implementation of the change resolved in adding a notification counter with a more vibrant color. The reason for implementing this change is that if users don't realize they have notifications, the message in the notification is lost and could lead to i.e. a user not remembering to do the daily notification, or that he has changed his team. This was the case in the usability test, which lead to one participant failing to complete the task. 
\begin{figure}[H]
    \centering
        \includegraphics[width=\textwidth]{task22improvement}
    \caption{Task 2.2 changes and justification}
    \label{task22improvement}
\end{figure}

\subsubsection*{Task 3.3: Find the mood-graph.} 
This recommendation, it's justification and it's priority(Severity) is presented in Figure \ref{task33improvement}. 
This recommendation was implemented by more clearly dividing the two different mood-graphs. This meant changing labels to implify what section they were connected to, and making the user more aware of what section they were in, and what tag-cloud they were looking at. 
\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{task33improvement}
    \caption{Task 3.3 changes and justification}
    \label{task33improvement}
\end{figure}

\subsubsection*{Task 5.1: Find all milestones for the current active repository.}
This recommendation, it's justification and it's priority(Severity) is presented in Figure \ref{task51improvement}. This improvement concerned missing feedback, when no milestones were made for a project. This lead to one participant failing to complete the task and therefore had a high priority. The improvement was implemented by adding a check to provide the user with appropriate feedback, if the chosen project had no milestones. 
\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{task51improvement}
    \caption{Task 5.1 changes and justification}
    \label{task51improvement}
\end{figure}

\subsubsection*{Task 5.3: Find repository related issues.} 
This recommendation, it's justification and it's priority(Severity) is presented in Figure \ref{task53improvement}. Task 5.3 generated the most errors, and also two critical errors leading to failure of completion. Therefore the improvement was given a high priority, and was implemented by clearly distincting milestone issues and the more general repository issues. We moved milestone issues to be more clearly visible under the \emph{Milestone sub-menu}, and added the repository issues under the \emph{Repository sub-menu}. 
\begin{figure}[H]
    \centering
        \includegraphics[width=\textwidth]{task53improvement}
    \caption{Task 5.3 changes and justification}
    \label{task53improvement}
\end{figure}

\subsection*{Unimplemented Changes}
This section presents the proposed changes which were not implemented to the PeacefulBanana application, and their justifications are presented in Figure \ref{unshareimprovement} and Figure \ref{commitimprovement}. The \emph{unshare functionality change} was omitted, since allowing user's to change their reflection outcomes a long time after they occured, would not improve reflection for user's but instead obscure the outcome. The \emph{commit impact change} was omitted, because of it's low priority. User's generally had a bad attitude towards what the commit impact captured, but reported they would rather have it there than not, as long as the team is aware that it might not reflect the correct project activity. 
\subsubsection*{Add unshare functionality}
\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{unshareimprovement}
    \caption{Adding unshare functionality to reflection notes.}
    \label{unshareimprovement}
\end{figure}

\subsubsection*{Remove commit impact}
\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{commitimprovement}
    \caption{Remove commit impact.}
    \label{commitimprovement}
\end{figure}

\subsection{Conclusion}
Most of the participants found the PeacefulBanana application to be well-organized, comprehensive, clean and uncluttered, very useful, and easy to use. Having an application to handle reflection in their daily work was desirable for all of the participants. Implementing the recommendations and further feedback from users will ensure a continued user-friendly application.

While some tasks had a high time-on-task, this was due to GitHub data collection, and is not something we have control over. It is a simple request to the GitHub API, which we need to receive before being able to continue. The application does not freeze during this wait, and allows users to continue using it, but the data collected is required for certain parts of the functionality.  
